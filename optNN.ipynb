{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datagenerator import min_len, max_len\n",
    "num_of_diff_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will one hot encode the data. the input and output will have different length of one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 15, 10) (150000, 15)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.zeros((len(data), max_len, num_of_diff_num)) # shape 20000x10. 10 is because the value in seq is 1 0 - 9\n",
    "y_train = np.zeros((len(data), max_len)) # shape 20000x30. 30 is because max len of seq is 30\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seqs = []\n",
    "page_faults = []\n",
    "for i in range(len(data)):\n",
    "    seqs.append(data[i]['seq'])\n",
    "    page_faults.append(data[i]['page_faults'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(seqs):\n",
    "    for j, val in enumerate(seq):\n",
    "        x_train[i, j, val] = 1\n",
    "\n",
    "for i, pf in enumerate(page_faults):\n",
    "    y_train[i, pf] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 15, 128)           71168     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                975       \n",
      "=================================================================\n",
      "Total params: 121,551\n",
      "Trainable params: 121,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True, input_shape=(max_len, num_of_diff_num)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, activation='relu', input_shape=(max_len,num_of_diff_num)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(max_len, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/100\n",
      "120000/120000 [==============================] - 82s - loss: 1.6742 - acc: 0.3110 - val_loss: 1.6042 - val_acc: 0.3301\n",
      "Epoch 2/100\n",
      "120000/120000 [==============================] - 81s - loss: 1.6048 - acc: 0.3297 - val_loss: 1.5703 - val_acc: 0.3613\n",
      "Epoch 3/100\n",
      "120000/120000 [==============================] - 83s - loss: 1.1756 - acc: 0.4784 - val_loss: 0.8802 - val_acc: 0.6046\n",
      "Epoch 4/100\n",
      "120000/120000 [==============================] - 85s - loss: 0.9111 - acc: 0.5948 - val_loss: 0.7959 - val_acc: 0.6494\n",
      "Epoch 5/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.8188 - acc: 0.6353 - val_loss: 0.7247 - val_acc: 0.6776\n",
      "Epoch 6/100\n",
      "120000/120000 [==============================] - 85s - loss: 0.7604 - acc: 0.6641 - val_loss: 0.6738 - val_acc: 0.7048\n",
      "Epoch 7/100\n",
      "120000/120000 [==============================] - 85s - loss: 0.7220 - acc: 0.6835 - val_loss: 0.6727 - val_acc: 0.7110\n",
      "Epoch 8/100\n",
      "120000/120000 [==============================] - 88s - loss: 0.6963 - acc: 0.6949 - val_loss: 0.6155 - val_acc: 0.7301\n",
      "Epoch 9/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.6739 - acc: 0.7072 - val_loss: 0.5807 - val_acc: 0.7529\n",
      "Epoch 10/100\n",
      "120000/120000 [==============================] - 85s - loss: 0.6545 - acc: 0.7151 - val_loss: 0.5857 - val_acc: 0.7501\n",
      "Epoch 11/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.6413 - acc: 0.7225 - val_loss: 0.5693 - val_acc: 0.7577\n",
      "Epoch 12/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.6254 - acc: 0.7289 - val_loss: 0.5768 - val_acc: 0.7471\n",
      "Epoch 13/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.6160 - acc: 0.7317 - val_loss: 0.5878 - val_acc: 0.7502\n",
      "Epoch 14/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.6039 - acc: 0.7383 - val_loss: 0.5476 - val_acc: 0.7616\n",
      "Epoch 15/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.5933 - acc: 0.7433 - val_loss: 0.5425 - val_acc: 0.7629\n",
      "Epoch 16/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.5820 - acc: 0.7508 - val_loss: 0.5311 - val_acc: 0.7741\n",
      "Epoch 17/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.5785 - acc: 0.7512 - val_loss: 0.5571 - val_acc: 0.7606\n",
      "Epoch 18/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.5680 - acc: 0.7574 - val_loss: 0.5392 - val_acc: 0.7638\n",
      "Epoch 19/100\n",
      "120000/120000 [==============================] - 83s - loss: 0.5583 - acc: 0.7607 - val_loss: 0.5314 - val_acc: 0.7731\n",
      "Epoch 20/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.5494 - acc: 0.7652 - val_loss: 0.5561 - val_acc: 0.7529\n",
      "Epoch 21/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.5436 - acc: 0.7674 - val_loss: 0.5375 - val_acc: 0.7678\n",
      "Epoch 22/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.5350 - acc: 0.7719 - val_loss: 0.5144 - val_acc: 0.7820\n",
      "Epoch 23/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.5299 - acc: 0.7754 - val_loss: 0.4964 - val_acc: 0.7896\n",
      "Epoch 24/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.5147 - acc: 0.7822 - val_loss: 0.4966 - val_acc: 0.7908\n",
      "Epoch 25/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.5086 - acc: 0.7850 - val_loss: 0.4890 - val_acc: 0.7905\n",
      "Epoch 26/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.5022 - acc: 0.7884 - val_loss: 0.4842 - val_acc: 0.7928\n",
      "Epoch 27/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4976 - acc: 0.7897 - val_loss: 0.4754 - val_acc: 0.7960\n",
      "Epoch 28/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4855 - acc: 0.7957 - val_loss: 0.4767 - val_acc: 0.7930\n",
      "Epoch 29/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4780 - acc: 0.7995 - val_loss: 0.4587 - val_acc: 0.8042\n",
      "Epoch 30/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4683 - acc: 0.8034 - val_loss: 0.4600 - val_acc: 0.8051\n",
      "Epoch 31/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4591 - acc: 0.8086 - val_loss: 0.4471 - val_acc: 0.8143\n",
      "Epoch 32/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4523 - acc: 0.8122 - val_loss: 0.4448 - val_acc: 0.8113\n",
      "Epoch 33/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4446 - acc: 0.8166 - val_loss: 0.4283 - val_acc: 0.8213\n",
      "Epoch 34/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4393 - acc: 0.8167 - val_loss: 0.4164 - val_acc: 0.8262\n",
      "Epoch 35/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.4274 - acc: 0.8237 - val_loss: 0.4174 - val_acc: 0.8265\n",
      "Epoch 36/100\n",
      "120000/120000 [==============================] - 79s - loss: 0.4143 - acc: 0.8299 - val_loss: 0.4023 - val_acc: 0.8316\n",
      "Epoch 37/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.4056 - acc: 0.8346 - val_loss: 0.4017 - val_acc: 0.8306\n",
      "Epoch 38/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.3987 - acc: 0.8381 - val_loss: 0.3806 - val_acc: 0.8437\n",
      "Epoch 39/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.3892 - acc: 0.8423 - val_loss: 0.3713 - val_acc: 0.8495\n",
      "Epoch 40/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.3811 - acc: 0.8473 - val_loss: 0.3977 - val_acc: 0.8323\n",
      "Epoch 41/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.3717 - acc: 0.8506 - val_loss: 0.4071 - val_acc: 0.8352\n",
      "Epoch 42/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.3650 - acc: 0.8533 - val_loss: 0.3811 - val_acc: 0.8469\n",
      "Epoch 43/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3549 - acc: 0.8575 - val_loss: 0.3336 - val_acc: 0.8658\n",
      "Epoch 44/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3492 - acc: 0.8610 - val_loss: 0.3295 - val_acc: 0.8667\n",
      "Epoch 45/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3390 - acc: 0.8645 - val_loss: 0.3294 - val_acc: 0.8632\n",
      "Epoch 46/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3275 - acc: 0.8700 - val_loss: 0.3140 - val_acc: 0.8740\n",
      "Epoch 47/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3237 - acc: 0.8719 - val_loss: 0.3242 - val_acc: 0.8704\n",
      "Epoch 48/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3266 - acc: 0.8708 - val_loss: 0.3059 - val_acc: 0.8776\n",
      "Epoch 49/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3118 - acc: 0.8770 - val_loss: 0.2945 - val_acc: 0.8825\n",
      "Epoch 50/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3063 - acc: 0.8805 - val_loss: 0.2907 - val_acc: 0.8846\n",
      "Epoch 51/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.3006 - acc: 0.8827 - val_loss: 0.2978 - val_acc: 0.8837\n",
      "Epoch 52/100\n",
      "120000/120000 [==============================] - 87s - loss: 0.2957 - acc: 0.8846 - val_loss: 0.2890 - val_acc: 0.8879\n",
      "Epoch 53/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2888 - acc: 0.8873 - val_loss: 0.2949 - val_acc: 0.8808\n",
      "Epoch 54/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2824 - acc: 0.8896 - val_loss: 0.2760 - val_acc: 0.8906\n",
      "Epoch 55/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2777 - acc: 0.8924 - val_loss: 0.2808 - val_acc: 0.8876\n",
      "Epoch 56/100\n",
      "120000/120000 [==============================] - 85s - loss: 0.2716 - acc: 0.8950 - val_loss: 0.2675 - val_acc: 0.8944\n",
      "Epoch 57/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2651 - acc: 0.8970 - val_loss: 0.2665 - val_acc: 0.8949\n",
      "Epoch 58/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2671 - acc: 0.8975 - val_loss: 0.2551 - val_acc: 0.9006\n",
      "Epoch 59/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2590 - acc: 0.9002 - val_loss: 0.2695 - val_acc: 0.8974\n",
      "Epoch 60/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2527 - acc: 0.9025 - val_loss: 0.2474 - val_acc: 0.9037\n",
      "Epoch 61/100\n",
      "120000/120000 [==============================] - 84s - loss: 0.2476 - acc: 0.9042 - val_loss: 0.2415 - val_acc: 0.9079\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000/120000 [==============================] - 81s - loss: 0.2447 - acc: 0.9059 - val_loss: 0.2565 - val_acc: 0.8987\n",
      "Epoch 63/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.2407 - acc: 0.9080 - val_loss: 0.2425 - val_acc: 0.9056\n",
      "Epoch 64/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.2355 - acc: 0.9119 - val_loss: 0.2292 - val_acc: 0.9110\n",
      "Epoch 65/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2318 - acc: 0.9116 - val_loss: 0.2914 - val_acc: 0.8826\n",
      "Epoch 66/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2281 - acc: 0.9137 - val_loss: 0.2488 - val_acc: 0.9036\n",
      "Epoch 67/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.2224 - acc: 0.9162 - val_loss: 0.2425 - val_acc: 0.9035\n",
      "Epoch 68/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2200 - acc: 0.9173 - val_loss: 0.2303 - val_acc: 0.9108\n",
      "Epoch 69/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2186 - acc: 0.9175 - val_loss: 0.2334 - val_acc: 0.9094\n",
      "Epoch 70/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2135 - acc: 0.9191 - val_loss: 0.2484 - val_acc: 0.9035\n",
      "Epoch 71/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.2108 - acc: 0.9210 - val_loss: 0.2633 - val_acc: 0.9050\n",
      "Epoch 72/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2066 - acc: 0.9227 - val_loss: 0.2263 - val_acc: 0.9146\n",
      "Epoch 73/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.2037 - acc: 0.9229 - val_loss: 0.2245 - val_acc: 0.9134\n",
      "Epoch 74/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.2035 - acc: 0.9236 - val_loss: 0.2407 - val_acc: 0.9109\n",
      "Epoch 75/100\n",
      "120000/120000 [==============================] - 80s - loss: 0.1978 - acc: 0.9267 - val_loss: 0.2199 - val_acc: 0.9160\n",
      "Epoch 76/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1975 - acc: 0.9269 - val_loss: 0.2221 - val_acc: 0.9167\n",
      "Epoch 77/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1936 - acc: 0.9273 - val_loss: 0.2314 - val_acc: 0.9118\n",
      "Epoch 78/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1927 - acc: 0.9287 - val_loss: 0.2221 - val_acc: 0.9138\n",
      "Epoch 79/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1922 - acc: 0.9292 - val_loss: 0.2155 - val_acc: 0.9196\n",
      "Epoch 80/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1837 - acc: 0.9323 - val_loss: 0.2202 - val_acc: 0.9146\n",
      "Epoch 81/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1856 - acc: 0.9318 - val_loss: 0.2059 - val_acc: 0.9224\n",
      "Epoch 82/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1798 - acc: 0.9343 - val_loss: 0.2027 - val_acc: 0.9231\n",
      "Epoch 83/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1781 - acc: 0.9349 - val_loss: 0.1984 - val_acc: 0.9254\n",
      "Epoch 84/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1758 - acc: 0.9351 - val_loss: 0.2151 - val_acc: 0.9205\n",
      "Epoch 85/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1724 - acc: 0.9370 - val_loss: 0.2142 - val_acc: 0.9217\n",
      "Epoch 86/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1691 - acc: 0.9373 - val_loss: 0.2072 - val_acc: 0.9237\n",
      "Epoch 87/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1692 - acc: 0.9383 - val_loss: 0.2304 - val_acc: 0.9127\n",
      "Epoch 88/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1662 - acc: 0.9396 - val_loss: 0.1970 - val_acc: 0.9262\n",
      "Epoch 89/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.1672 - acc: 0.9386 - val_loss: 0.2115 - val_acc: 0.9200\n",
      "Epoch 90/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1632 - acc: 0.9399 - val_loss: 0.2005 - val_acc: 0.9263\n",
      "Epoch 91/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1614 - acc: 0.9417 - val_loss: 0.1929 - val_acc: 0.9290\n",
      "Epoch 92/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1610 - acc: 0.9414 - val_loss: 0.2002 - val_acc: 0.9273\n",
      "Epoch 93/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1578 - acc: 0.9430 - val_loss: 0.1916 - val_acc: 0.9307\n",
      "Epoch 94/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1532 - acc: 0.9453 - val_loss: 0.1947 - val_acc: 0.9264\n",
      "Epoch 95/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1485 - acc: 0.9463 - val_loss: 0.1933 - val_acc: 0.9306\n",
      "Epoch 96/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.1497 - acc: 0.9466 - val_loss: 0.2045 - val_acc: 0.9238\n",
      "Epoch 97/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1516 - acc: 0.9453 - val_loss: 0.2199 - val_acc: 0.9203\n",
      "Epoch 98/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.1462 - acc: 0.9474 - val_loss: 0.1931 - val_acc: 0.9300\n",
      "Epoch 99/100\n",
      "120000/120000 [==============================] - 81s - loss: 0.1496 - acc: 0.9464 - val_loss: 0.1821 - val_acc: 0.9333\n",
      "Epoch 100/100\n",
      "120000/120000 [==============================] - 82s - loss: 0.1427 - acc: 0.9481 - val_loss: 0.1908 - val_acc: 0.9303\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=100, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`save_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a13a50e1e0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optNN_v1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \"\"\"\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_model` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_json_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_model` requires h5py."
     ]
    }
   ],
   "source": [
    "model.save('optNN_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
